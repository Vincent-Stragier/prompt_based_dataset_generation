{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "def rouge_score(generation, reference):\n",
    "    return rouge_scorer.get_scores(\n",
    "        hyps=generation,\n",
    "        refs=reference,\n",
    "    )[0][\"rouge-l\"][\"f\"]\n",
    "# print(rouge_score(hypothesis, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of txt files:  ['.\\\\datasets\\\\results_prompts_0\\\\0.txt', '.\\\\datasets\\\\results_prompts_0\\\\1.txt', '.\\\\datasets\\\\results_prompts_0\\\\10.txt', '.\\\\datasets\\\\results_prompts_0\\\\11.txt', '.\\\\datasets\\\\results_prompts_0\\\\12.txt', '.\\\\datasets\\\\results_prompts_0\\\\13.txt', '.\\\\datasets\\\\results_prompts_0\\\\14.txt', '.\\\\datasets\\\\results_prompts_0\\\\15.txt', '.\\\\datasets\\\\results_prompts_0\\\\16.txt', '.\\\\datasets\\\\results_prompts_0\\\\17.txt', '.\\\\datasets\\\\results_prompts_0\\\\2.txt', '.\\\\datasets\\\\results_prompts_0\\\\3.txt', '.\\\\datasets\\\\results_prompts_0\\\\4.txt', '.\\\\datasets\\\\results_prompts_0\\\\5.txt', '.\\\\datasets\\\\results_prompts_0\\\\6.txt', '.\\\\datasets\\\\results_prompts_0\\\\7.txt', '.\\\\datasets\\\\results_prompts_0\\\\8.txt', '.\\\\datasets\\\\results_prompts_0\\\\9.txt', '.\\\\datasets\\\\results_prompts_1\\\\0.txt', '.\\\\datasets\\\\results_prompts_1\\\\1.txt', '.\\\\datasets\\\\results_prompts_1\\\\10.txt', '.\\\\datasets\\\\results_prompts_1\\\\11.txt', '.\\\\datasets\\\\results_prompts_1\\\\12.txt', '.\\\\datasets\\\\results_prompts_1\\\\13.txt', '.\\\\datasets\\\\results_prompts_1\\\\14.txt', '.\\\\datasets\\\\results_prompts_1\\\\15.txt', '.\\\\datasets\\\\results_prompts_1\\\\16.txt', '.\\\\datasets\\\\results_prompts_1\\\\17.txt', '.\\\\datasets\\\\results_prompts_1\\\\2.txt', '.\\\\datasets\\\\results_prompts_1\\\\3.txt', '.\\\\datasets\\\\results_prompts_1\\\\4.txt', '.\\\\datasets\\\\results_prompts_1\\\\5.txt', '.\\\\datasets\\\\results_prompts_1\\\\6.txt', '.\\\\datasets\\\\results_prompts_1\\\\7.txt', '.\\\\datasets\\\\results_prompts_1\\\\8.txt', '.\\\\datasets\\\\results_prompts_1\\\\9.txt']\n"
     ]
    }
   ],
   "source": [
    "# List all txt files in subdirectories\n",
    "txt_files = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Number of txt files: \", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the first file\n",
    "\n",
    "def validate_line(line):\n",
    "    # Check if the line is a valid json\n",
    "    try:        \n",
    "        # Add the brackets to make it a valid json\n",
    "        line = f\"[{line}]\"\n",
    "        \n",
    "        # Remove the last comma\n",
    "        line = re.sub(r\"\\,\\s*\\]\", r\"]\", line, 1)\n",
    "\n",
    "        json.loads(line)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_json(filename):\n",
    "    # print(filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text[max(text.find('['), 0):]\n",
    "\n",
    "        # Remove the </s> tag, [ and ] from the text\n",
    "        text = text.replace(r\"</s>\",\"\")\n",
    "        text = text.replace(r\"[\",\"\")\n",
    "        text = text.replace(r\"]\",\"\")\n",
    "        text = text.replace(r\"\\n\",\"\")\n",
    "        \n",
    "        # Remove malformed json elements\n",
    "        text = re.sub(r\"(\\}\\,\\s*\\w+\\s*\\{)\", r\"},\\n{\", text)\n",
    "        \n",
    "        text = filter(validate_line, text.split(\"\\n\"))\n",
    "        text = \"\\n\".join(text)\n",
    "        \n",
    "        # Add the brackets to make it a valid json\n",
    "        text = f\"[{text}]\"\n",
    "        \n",
    "        # Remove the last comma\n",
    "        text = re.sub(r\"\\,\\s*\\]\", r\"]\", text, 1)\n",
    "        \n",
    "        # # Remove potential last malformed json elements\n",
    "        # text = re.sub(r\"(\\,\\s*\\{[\\w\\s\\\":'\\(\\)\\_\\.\\,]*\\])\", r\"]\", text)\n",
    "\n",
    "        return json.loads(text)\n",
    "\n",
    "# for file in txt_files:\n",
    "#     print(extract_json(file))\n",
    "    \n",
    "def natural_sort(list_to_sort): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    return sorted(list_to_sort.copy(), key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_FILE = \"tools/tools.json\"\n",
    "DATASET_FILES_0 = \"datasets/results_prompts_0\"\n",
    "DATASET_FILES_1 = \"datasets/results_prompts_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Open tool json file\n",
    "with open(TOOLS_FILE, 'r') as json_file:\n",
    "    tools = json.load(json_file)\n",
    "\n",
    "def extract_dataset(dataset_files, tools):\n",
    "    dataset = []\n",
    "\n",
    "    for file in natural_sort(dataset_files):\n",
    "        dataset.append(extract_json(file))\n",
    "\n",
    "    new_dataset = []\n",
    "    for data, tool in zip(dataset, tools):\n",
    "        new_entry = tool\n",
    "        reference = tool['use_cases'][0]['user_request']\n",
    "        new_data = []\n",
    "        \n",
    "        for generation in data:\n",
    "            scored_generation = generation\n",
    "            user_request = scored_generation['user_request']\n",
    "            # print(rouge_score(user_request, reference))\n",
    "            scored_generation.update({\"rouge_score\": rouge_score(user_request, reference)})\n",
    "            new_data.append(scored_generation)\n",
    "\n",
    "        new_entry.update({\"dataset\": new_data})\n",
    "        new_dataset.append(new_entry)\n",
    "\n",
    "    return deepcopy(new_dataset)\n",
    "\n",
    "def make_json(data, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)\n",
    "\n",
    "# print(list(f\"{DATASET_FILES_0}/{file}\" for file in os.listdir(DATASET_FILES_0)))\n",
    "dataset_files_0 = [f\"{DATASET_FILES_0}/{file}\" for file in os.listdir(DATASET_FILES_0)]\n",
    "dataset_files_1 = [f\"{DATASET_FILES_1}/{file}\" for file in os.listdir(DATASET_FILES_1)]\n",
    "\n",
    "dataset_0 = extract_dataset(dataset_files_0, tools)\n",
    "dataset_1 = extract_dataset(dataset_files_1, tools)\n",
    "\n",
    "make_json(dataset_0, f\"{DATASET_FILES_0}.json\")\n",
    "make_json(dataset_1, f\"{DATASET_FILES_1}.json\")\n",
    "\n",
    "print(extract_dataset(dataset_files_1, tools) == extract_dataset(dataset_files_0, tools))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the datasets with BLUE and ROUGE\n",
    "1. Load the datasets\n",
    "2. Compute the BLUE and ROUGE scores\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in c:\\users\\vincent\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823479584776\n",
      "Is there a chair in the room?\n",
      "Is there a sink in the room?\n",
      "0.8571428521428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hypothesis = \"to make people trustworthy you need to trust them\"\n",
    "# reference = \"the way to make people trustworthy is to trust them\"\n",
    "\n",
    "\n",
    "\n",
    "# reference = dataset_0[0]['use_cases'][0]['user_request']\n",
    "# generation = dataset_0[0]['dataset'][]['user_request']\n",
    "# print(reference)\n",
    "# print(generation)\n",
    "\n",
    "# #  > 0.999999\n",
    "# print(rouge_score(generation, reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the dataset\n",
    "1. Load the dataset\n",
    "2. Compute the number of generation by tool and the mean rouge score (for each tool and for all tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_mean_rouge(dataset):\n",
    "    total_average_rouge_score = 0\n",
    "    total_number_of_generations = 0\n",
    "\n",
    "    for data in dataset:    \n",
    "        classe = data['dataset']\n",
    "        average_rouge_score = 0\n",
    "\n",
    "        for generation in classe:\n",
    "            average_rouge_score += generation['rouge_score']\n",
    "            \n",
    "        average_rouge_score /= len(classe)\n",
    "        total_average_rouge_score += average_rouge_score\n",
    "        total_number_of_generations += len(classe)\n",
    "        \n",
    "        print(\"Class:\", data['tool_name'], \"\\taverage ROUGE score:\", average_rouge_score)\n",
    "\n",
    "    total_average_rouge_score /= total_number_of_generations\n",
    "    print(\"Total average ROUGE score:\", total_average_rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe: detect_object \taverage ROUGE score: 0.38469106173714723\n",
      "Classe: enumerate_objects \taverage ROUGE score: 0.3301340027102317\n",
      "Classe: navigation \taverage ROUGE score: 0.516528920634861\n",
      "Classe: position \taverage ROUGE score: 0.010416666471354169\n",
      "Classe: add_face \taverage ROUGE score: 0.05291005201058202\n",
      "Classe: remove_face \taverage ROUGE score: 0.06144781029461283\n",
      "Classe: look_for_face \taverage ROUGE score: 0.010101009898989903\n",
      "Classe: enumerate_individuals \taverage ROUGE score: 0.0\n",
      "Classe: age_estimation \taverage ROUGE score: 0.15151514904109012\n",
      "Classe: gender_estimation \taverage ROUGE score: 0.06593406501567235\n",
      "Classe: emotion_estimation \taverage ROUGE score: 0.09256198094665673\n",
      "Classe: colors \taverage ROUGE score: 0.32435640436759766\n",
      "Classe: object_color \taverage ROUGE score: 0.4510207879933894\n",
      "Classe: ocr \taverage ROUGE score: 0.2859504082268971\n",
      "Classe: ocr_objects \taverage ROUGE score: 0.2684777295577613\n",
      "Classe: money \taverage ROUGE score: 0.17371845877603714\n",
      "Classe: environment_description \taverage ROUGE score: 0.36868686421780306\n",
      "Classe: environment_question \taverage ROUGE score: 0.006993006777837554\n",
      "Total average ROUGE score: 0.006971459566036315\n"
     ]
    }
   ],
   "source": [
    "print_dataset_mean_rouge(dataset_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe: detect_object \taverage ROUGE score: 0.8560846510871253\n",
      "Classe: enumerate_objects \taverage ROUGE score: 0.46048647825218103\n",
      "Classe: navigation \taverage ROUGE score: 0.5571747043697814\n",
      "Classe: position \taverage ROUGE score: 0.03571428526785715\n",
      "Classe: add_face \taverage ROUGE score: 0.4015567715959727\n",
      "Classe: remove_face \taverage ROUGE score: 0.4032633982902994\n",
      "Classe: look_for_face \taverage ROUGE score: 0.37996031254269425\n",
      "Classe: enumerate_individuals \taverage ROUGE score: 0.020833333138020837\n",
      "Classe: age_estimation \taverage ROUGE score: 0.36381673412765986\n",
      "Classe: gender_estimation \taverage ROUGE score: 0.15120772778003222\n",
      "Classe: emotion_estimation \taverage ROUGE score: 0.10148336264821496\n",
      "Classe: colors \taverage ROUGE score: 0.60030120898732\n",
      "Classe: object_color \taverage ROUGE score: 0.7677018583550037\n",
      "Classe: ocr \taverage ROUGE score: 0.7867768545116455\n",
      "Classe: ocr_objects \taverage ROUGE score: 0.3409951110237474\n",
      "Classe: money \taverage ROUGE score: 0.1476967451334499\n",
      "Classe: environment_description \taverage ROUGE score: 0.3332644585059275\n",
      "Classe: environment_question \taverage ROUGE score: 0.08608058355964912\n",
      "Total average ROUGE score: 0.015031855263665003\n"
     ]
    }
   ],
   "source": [
    "print_dataset_mean_rouge(dataset_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select n generation randomly in each class of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# random.seed(42)\n",
    "random.seed(None)\n",
    "\n",
    "def lower_case_first_letter(string):\n",
    "    return string[0].lower() + string[1:]\n",
    "\n",
    "def create_an_enumeration(list_of_strings):\n",
    "    return \"\\n\".join([f\"{i+1}. {string}\" for i, string in enumerate(list_of_strings)])\n",
    "\n",
    "def randomly_select_n_generations_for_a_specific_tool(tool, n: int):\n",
    "    generations = tool['dataset']\n",
    "    random.shuffle(generations)\n",
    "    return deepcopy(generations[:n])\n",
    "\n",
    "def randomly_select_n_generations(dataset, n: int):\n",
    "    for tool in dataset:\n",
    "        generations = randomly_select_n_generations_for_a_specific_tool(tool, n)\n",
    "        list_of_generations = [generation['user_request'] for generation in generations]\n",
    "        print(f\"\"\"The tool '{tool['tool_name']}' should {lower_case_first_letter(tool['description'])}'\n",
    "{create_an_enumeration(list_of_generations)}\"\"\")\n",
    "        \n",
    "# randomly_select_n_generations(dataset_0, 5)\n",
    "\n",
    "def make_description(dataset_a, dataset_b, n: int = 5, filename: str = \"form.txt\"):\n",
    "    tools_questions = []\n",
    "    for tool_a, tool_b in zip(dataset_a, dataset_b):\n",
    "        random_generations_a = randomly_select_n_generations_for_a_specific_tool(tool_a, n)\n",
    "        random_generations_b = randomly_select_n_generations_for_a_specific_tool(tool_b, n)\n",
    "        list_of_generations_a = [generation['user_request'] for generation in random_generations_a]\n",
    "        list_of_generations_b = [generation['user_request'] for generation in random_generations_b]\n",
    "        enumerated_generations_a = create_an_enumeration(list_of_generations_a)\n",
    "        enumerated_generations_b = create_an_enumeration(list_of_generations_b)\n",
    "\n",
    "        tools_questions.append(f\"\"\"The tool '{tool_a['tool_name']}' should {lower_case_first_letter(tool_a['description'])}\n",
    "In the dataset a, the following user requests were generated for the tool '{tool_a['tool_name']}':\n",
    "\n",
    "{enumerated_generations_a}\n",
    "\n",
    "In the dataset b, the following user requests were generated for the tool '{tool_a['tool_name']}':\n",
    "\n",
    "{enumerated_generations_b}\n",
    "\"\"\")\n",
    "    with open(filename, 'w') as text_file:\n",
    "        text_file.write(\"\\n\".join(tools_questions))\n",
    "\n",
    "make_description(dataset_0, dataset_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
