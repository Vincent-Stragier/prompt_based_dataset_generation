{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "def rouge_score(generation, reference):\n",
    "    return rouge_scorer.get_scores(\n",
    "        hyps=generation,\n",
    "        refs=reference,\n",
    "    )[0][\"rouge-l\"][\"f\"]\n",
    "# print(rouge_score(hypothesis, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of txt files:  ['.\\\\datasets\\\\results_prompts_0\\\\0.txt', '.\\\\datasets\\\\results_prompts_0\\\\1.txt', '.\\\\datasets\\\\results_prompts_0\\\\10.txt', '.\\\\datasets\\\\results_prompts_0\\\\11.txt', '.\\\\datasets\\\\results_prompts_0\\\\12.txt', '.\\\\datasets\\\\results_prompts_0\\\\13.txt', '.\\\\datasets\\\\results_prompts_0\\\\14.txt', '.\\\\datasets\\\\results_prompts_0\\\\15.txt', '.\\\\datasets\\\\results_prompts_0\\\\16.txt', '.\\\\datasets\\\\results_prompts_0\\\\17.txt', '.\\\\datasets\\\\results_prompts_0\\\\2.txt', '.\\\\datasets\\\\results_prompts_0\\\\3.txt', '.\\\\datasets\\\\results_prompts_0\\\\4.txt', '.\\\\datasets\\\\results_prompts_0\\\\5.txt', '.\\\\datasets\\\\results_prompts_0\\\\6.txt', '.\\\\datasets\\\\results_prompts_0\\\\7.txt', '.\\\\datasets\\\\results_prompts_0\\\\8.txt', '.\\\\datasets\\\\results_prompts_0\\\\9.txt', '.\\\\datasets\\\\results_prompts_1\\\\0.txt', '.\\\\datasets\\\\results_prompts_1\\\\1.txt', '.\\\\datasets\\\\results_prompts_1\\\\10.txt', '.\\\\datasets\\\\results_prompts_1\\\\11.txt', '.\\\\datasets\\\\results_prompts_1\\\\12.txt', '.\\\\datasets\\\\results_prompts_1\\\\13.txt', '.\\\\datasets\\\\results_prompts_1\\\\14.txt', '.\\\\datasets\\\\results_prompts_1\\\\15.txt', '.\\\\datasets\\\\results_prompts_1\\\\16.txt', '.\\\\datasets\\\\results_prompts_1\\\\17.txt', '.\\\\datasets\\\\results_prompts_1\\\\2.txt', '.\\\\datasets\\\\results_prompts_1\\\\3.txt', '.\\\\datasets\\\\results_prompts_1\\\\4.txt', '.\\\\datasets\\\\results_prompts_1\\\\5.txt', '.\\\\datasets\\\\results_prompts_1\\\\6.txt', '.\\\\datasets\\\\results_prompts_1\\\\7.txt', '.\\\\datasets\\\\results_prompts_1\\\\8.txt', '.\\\\datasets\\\\results_prompts_1\\\\9.txt']\n"
     ]
    }
   ],
   "source": [
    "# List all txt files in subdirectories\n",
    "txt_files = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Number of txt files: \", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the first file\n",
    "\n",
    "def validate_line(line):\n",
    "    # Check if the line is a valid json\n",
    "    try:        \n",
    "        # Add the brackets to make it a valid json\n",
    "        line = f\"[{line}]\"\n",
    "        \n",
    "        # Remove the last comma\n",
    "        line = re.sub(r\"\\,\\s*\\]\", r\"]\", line, 1)\n",
    "\n",
    "        json.loads(line)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_json(filename):\n",
    "    print(filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text[max(text.find('['), 0):]\n",
    "\n",
    "        # Remove the </s> tag, [ and ] from the text\n",
    "        text = text.replace(r\"</s>\",\"\")\n",
    "        text = text.replace(r\"[\",\"\")\n",
    "        text = text.replace(r\"]\",\"\")\n",
    "        text = text.replace(r\"\\n\",\"\")\n",
    "        \n",
    "        # Remove malformed json elements\n",
    "        text = re.sub(r\"(\\}\\,\\s*\\w+\\s*\\{)\", r\"},\\n{\", text)\n",
    "        \n",
    "        text = filter(validate_line, text.split(\"\\n\"))\n",
    "        text = \"\\n\".join(text)\n",
    "        \n",
    "        # Add the brackets to make it a valid json\n",
    "        text = f\"[{text}]\"\n",
    "        \n",
    "        # Remove the last comma\n",
    "        text = re.sub(r\"\\,\\s*\\]\", r\"]\", text, 1)\n",
    "        \n",
    "        # # Remove potential last malformed json elements\n",
    "        # text = re.sub(r\"(\\,\\s*\\{[\\w\\s\\\":'\\(\\)\\_\\.\\,]*\\])\", r\"]\", text)\n",
    "\n",
    "        return json.loads(text)\n",
    "\n",
    "# for file in txt_files:\n",
    "#     print(extract_json(file))\n",
    "    \n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "# print(natural_sort(txt_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_FILE = \"tools/tools.json\"\n",
    "DATASET_FILES_0 = \"datasets/results_prompts_0\"\n",
    "DATASET_FILES_1 = \"datasets/results_prompts_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/results_prompts_1/0.txt\n",
      "datasets/results_prompts_1/1.txt\n",
      "datasets/results_prompts_1/2.txt\n",
      "datasets/results_prompts_1/3.txt\n",
      "datasets/results_prompts_1/4.txt\n",
      "datasets/results_prompts_1/5.txt\n",
      "datasets/results_prompts_1/6.txt\n",
      "datasets/results_prompts_1/7.txt\n",
      "datasets/results_prompts_1/8.txt\n",
      "datasets/results_prompts_1/9.txt\n",
      "datasets/results_prompts_1/10.txt\n",
      "datasets/results_prompts_1/11.txt\n",
      "datasets/results_prompts_1/12.txt\n",
      "datasets/results_prompts_1/13.txt\n",
      "datasets/results_prompts_1/14.txt\n",
      "datasets/results_prompts_1/15.txt\n",
      "datasets/results_prompts_1/16.txt\n",
      "datasets/results_prompts_1/17.txt\n"
     ]
    }
   ],
   "source": [
    "# Open tool json file\n",
    "with open(TOOLS_FILE, 'r') as json_file:\n",
    "    tools = json.load(json_file)\n",
    "\n",
    "def extract_dataset(dataset_files, tools):\n",
    "    dataset = []\n",
    "\n",
    "    for file in natural_sort(dataset_files):\n",
    "        dataset.append(extract_json(file))\n",
    "\n",
    "    new_dataset = []\n",
    "    for data, tool in zip(dataset, tools):\n",
    "        new_entry = tool\n",
    "        reference = tool['use_cases'][0]['user_request']\n",
    "        new_data = []\n",
    "        \n",
    "        for generation in data:\n",
    "            scored_generation = generation\n",
    "            user_request = scored_generation['user_request']\n",
    "            # print(rouge_score(user_request, reference))\n",
    "            scored_generation.update({\"rouge_score\": rouge_score(user_request, reference)})\n",
    "            new_data.append(scored_generation)\n",
    "\n",
    "        new_entry.update({\"dataset\": new_data})\n",
    "        new_dataset.append(new_entry)\n",
    "\n",
    "    return new_dataset\n",
    "\n",
    "def make_json(data, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)\n",
    "\n",
    "# print(list(f\"{DATASET_FILES_0}/{file}\" for file in os.listdir(DATASET_FILES_0)))\n",
    "dataset_files_0 = [f\"{DATASET_FILES_0}/{file}\" for file in os.listdir(DATASET_FILES_0)]\n",
    "dataset_files_1 = [f\"{DATASET_FILES_1}/{file}\" for file in os.listdir(DATASET_FILES_1)]\n",
    "\n",
    "# dataset_0 = extract_dataset(dataset_files_0, tools)\n",
    "dataset_1 = extract_dataset(dataset_files_1, tools)\n",
    "\n",
    "# make_json(dataset_0, f\"{DATASET_FILES_0}.json\")\n",
    "make_json(dataset_1, f\"{DATASET_FILES_1}.json\")\n",
    "\n",
    "# print(extract_dataset(dataset_files_1, tools) == extract_dataset(dataset_files_0, tools))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the datasets with BLUE and ROUGE\n",
    "1. Load the datasets\n",
    "2. Compute the BLUE and ROUGE scores\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in c:\\users\\vincent\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823479584776\n",
      "Is there a chair in the room?\n",
      "Is there a sink in the room?\n",
      "0.8571428521428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hypothesis = \"to make people trustworthy you need to trust them\"\n",
    "# reference = \"the way to make people trustworthy is to trust them\"\n",
    "\n",
    "\n",
    "\n",
    "# reference = dataset_0[0]['use_cases'][0]['user_request']\n",
    "# generation = dataset_0[0]['dataset'][]['user_request']\n",
    "# print(reference)\n",
    "# print(generation)\n",
    "\n",
    "# #  > 0.999999\n",
    "# print(rouge_score(generation, reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the dataset\n",
    "1. Load the dataset\n",
    "2. Compute the number of generation by tool and the mean rouge score (for each tool and for all tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe: detect_object\n",
      "Average ROUGE score: 0.8560846510871253\n",
      "\n",
      "\n",
      "Classe: enumerate_objects\n",
      "Average ROUGE score: 0.46048647825218103\n",
      "\n",
      "\n",
      "Classe: navigation\n",
      "Average ROUGE score: 0.5571747043697814\n",
      "\n",
      "\n",
      "Classe: position\n",
      "Average ROUGE score: 0.03571428526785715\n",
      "\n",
      "\n",
      "Classe: add_face\n",
      "Average ROUGE score: 0.4015567715959727\n",
      "\n",
      "\n",
      "Classe: remove_face\n",
      "Average ROUGE score: 0.4032633982902994\n",
      "\n",
      "\n",
      "Classe: look_for_face\n",
      "Average ROUGE score: 0.37996031254269425\n",
      "\n",
      "\n",
      "Classe: enumerate_individuals\n",
      "Average ROUGE score: 0.020833333138020837\n",
      "\n",
      "\n",
      "Classe: age_estimation\n",
      "Average ROUGE score: 0.36381673412765986\n",
      "\n",
      "\n",
      "Classe: gender_estimation\n",
      "Average ROUGE score: 0.15120772778003222\n",
      "\n",
      "\n",
      "Classe: emotion_estimation\n",
      "Average ROUGE score: 0.10148336264821496\n",
      "\n",
      "\n",
      "Classe: colors\n",
      "Average ROUGE score: 0.60030120898732\n",
      "\n",
      "\n",
      "Classe: object_color\n",
      "Average ROUGE score: 0.7677018583550037\n",
      "\n",
      "\n",
      "Classe: ocr\n",
      "Average ROUGE score: 0.7867768545116455\n",
      "\n",
      "\n",
      "Classe: ocr_objects\n",
      "Average ROUGE score: 0.3409951110237474\n",
      "\n",
      "\n",
      "Classe: money\n",
      "Average ROUGE score: 0.1476967451334499\n",
      "\n",
      "\n",
      "Classe: environment_description\n",
      "Average ROUGE score: 0.3332644585059275\n",
      "\n",
      "\n",
      "Classe: environment_question\n",
      "Average ROUGE score: 0.08608058355964912\n",
      "\n",
      "\n",
      "Total average ROUGE score: 0.015031855263665003\n"
     ]
    }
   ],
   "source": [
    "total_average_rouge_score = 0\n",
    "total_number_of_generations = 0\n",
    "\n",
    "for data in dataset_1:    \n",
    "    print(\"Classe:\", data['tool_name'])\n",
    "    classe = data['dataset']\n",
    "    average_rouge_score = 0\n",
    "\n",
    "    for generation in classe:\n",
    "        average_rouge_score += generation['rouge_score']\n",
    "        \n",
    "    average_rouge_score /= len(classe)\n",
    "    total_average_rouge_score += average_rouge_score\n",
    "    total_number_of_generations += len(classe)\n",
    "    \n",
    "    print(\"Average ROUGE score:\", average_rouge_score)\n",
    "    print(\"\\n\")\n",
    "\n",
    "total_average_rouge_score /= total_number_of_generations\n",
    "print(\"Total average ROUGE score:\", total_average_rouge_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
