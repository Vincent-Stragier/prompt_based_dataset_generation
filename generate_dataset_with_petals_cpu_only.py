# -*- coding: utf-8 -*-
"""generate_dataset_with_petals

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YKPlifrMiizKnZXyUDR3obFEW5VPXk-y

<div align="center">
<img src="https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67" width="40%">  
</div>

# Getting started with Petals

This notebook will guide you through the basics of Petals &mdash; a system for inference and fine-tuning large language models without the need to have high-end GPUs. With Petals, you can join compute resources with other people over the Internet and run large language models such as Llama 2 (70B), Stable Beluga 2, Llama-65B, Guanaco-65B, or BLOOM-176B right from your desktop computer or Google Colab.

💬 If you meet any issues while running this notebook, let us know in the **[#running-a-client](https://discord.gg/J29mCBNBvm)** channel of our Discord!

So, let's get started! First, let's install [the Petals package](https://github.com/bigscience-workshop/petals):
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install git+https://github.com/bigscience-workshop/petals

# !git clone https://github.com/Vincent-Stragier/prompt_based_dataset_generation

"""## Step 1. Loading the distributed model 🚀

Let's start with the easiest task &mdash; creating a distributed model and using it for generating text. This machine will download a small part of the model weights and rely on other computers in the network to run the rest of the model.

The Petals interface is compatible with PyTorch and the 🤗 [Transformers](https://github.com/huggingface/transformers) library &mdash; it feels like you're working with a local model even though its parts are hosted remotely. We suggest to start with [Stable Beluga 2 (70B)](https://huggingface.co/stabilityai/StableBeluga2), one of the best fine-tuned variants of Llama 2, but you can also use standard [Llama&nbsp;2&nbsp;(70B)](https://huggingface.co/meta-llama/Llama-2-70b-hf) if you have access to it (see below).
"""

# !huggingface-cli login


# model_name = "meta-llama/Llama-2-70b-chat-hf"
from tqdm import tqdm
import transformers
import os
import json
import torch
from transformers import AutoTokenizer
from petals import AutoDistributedModelForCausalLM
model_name = "petals-team/StableBeluga2"
# model_name = "tiiuae/falcon-180B-chat"
# You could also use "meta-llama/Llama-2-70b-chat-hf" or any other supported model from 🤗 Model Hub

tokenizer = AutoTokenizer.from_pretrained(
    model_name, use_fast=False, add_bos_token=False, torch_dtype=torch.float32)
model = AutoDistributedModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.float32)
# model = model.cuda()

"""🦙 **Want to run Llama 2?** Request access to its weights at the ♾️ [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and 🤗 [Model Hub](https://huggingface.co/meta-llama/Llama-2-70b-hf) (make sure to use the same email),  get an 🔑 [access token](https://huggingface.co/settings/tokens), then run `!huggingface-cli login --token YOUR_TOKEN` before loading the model. Or just try it in our [chatbot app](https://chat.petals.dev).

📋 **Friendly reminder.** This Colab is provided for demo purposes. If you want to use these models in your own projects, make sure you follow their terms of use (see the ones for [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt) and [Llama 2](https://bit.ly/llama2-license)).

### ✍️ How to generate text?

Let's try to generate something by calling __`model.generate()`__ method.

The first call to this method takes a few seconds to connect to the Petals swarm. Once we do that, you should expect generation speed of up to **5-6 tokens/sec**. If you don't have enough GPU memory to host the entire model, this is much faster than what you get with other methods, such as offloading or running the model on CPU.
"""


def extract_prompts(path: str) -> list:
    with open(path, mode='r', encoding='utf-8') as json_file:
        return json.load(json_file)


print(os.listdir('prompt_based_dataset_generation/tools'))
prompts_0 = extract_prompts(
    'prompt_based_dataset_generation/tools/prompts_0.json')
prompts_1 = extract_prompts(
    'prompt_based_dataset_generation/tools/prompts_1.json')

# prompts_1[0]

# mount it
# from google.colab import drive
# drive.mount('/content/drive')

DATA_DIR = f'~/thesis/2023/dataset_generation/{model_name}'
DATA_DIR = os.path.expanduser(DATA_DIR)
os.makedirs(DATA_DIR, mode=0o777, exist_ok=True)
os.listdir(DATA_DIR)


def iterative_json_writer(element, json_file, last: bool = False):
    """Write an element of a JSON file.

    Args:
        element (any): The element to write.
        json_file (file): The file to write to.
        last (bool, optional): Whether this is the last element. Defaults to False.
    """
    # Add indentation
    json_file.write("    ")

    # Write element
    json.dump(element, json_file, indent=4)

    # Add comma if not last element
    if not last:
        json_file.write(",")

    # Add newline
    json_file.write("\n")


if model_name == "tiiuae/falcon-180B-chat":
    pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map="auto",
    )


def generate_raw_dataset(input_data: list, output_file) -> None:
    prompts_outputs = []
    last_index = len(input_data) - 1

    with open(output_file, encoding='utf-8', mode='w') as json_file:
        json_file.write("[\n")

        for index, prompt in enumerate(tqdm(input_data)):
            # inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
            inputs = tokenizer(prompt, return_tensors="pt")["input_ids"]
            # print(inputs)
            outputs = model.generate(inputs, max_new_tokens=2000)
            # prompts_outputs.append(tokenizer.decode(outputs[0]))
            iterative_json_writer(tokenizer.decode(
                outputs[0]), json_file, index == last_index)
            # print(tokenizer.decode(outputs[0]), '\n\n')
        json_file.write("]\n")

    # sequences = pipeline(
    #   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    #     max_length=200,
    #     do_sample=True,
    #     top_k=10,
    #     num_return_sequences=1,
    #     eos_token_id=tokenizer.eos_token_id,
    # )
    # for seq in sequences:
    #     print(f"Result: {seq['generated_text']}")


generate_raw_dataset(prompts_0, f'{DATA_DIR}/results_prompts_0.json')
generate_raw_dataset(prompts_1, f'{DATA_DIR}/results_prompts_1.json')
